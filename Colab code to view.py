# -*- coding: utf-8 -*-
"""plsgod6 (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j4_7iffNkCP-RZiA5nNKQ2O2nSCtBg26
"""

!pip install fastapi
!pip install uvicorn
!pip install nest-asyncio
!git clone https://github.com/FasterDecoding/Medusa.git
!cd Medusa && pip install -e .

import os
os.chdir('Medusa')
!pip install pyngrok transformers accelerate bitsandbytes

import nest_asyncio
nest_asyncio.apply()

# need to add 2_ to two files inside model
# look for is_flash_attn_available in modelling_llama_kv.py, modelling_mistral_kv.py
# is_flash_attn_available -> is_flash_attn_2_available to fix the import error.

from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
import asyncio
from asyncio import Queue
from concurrent.futures import ThreadPoolExecutor
import torch
import numpy as np
from dataclasses import dataclass
import time
from transformers import BitsAndBytesConfig
from medusa.model.medusa_model import MedusaModel
from medusa.model.medusa_choices import mc_sim_7b_63
import os
from pyngrok import ngrok
from huggingface_hub import hf_hub_download
import uvicorn
from queue import Empty

# Cell 3 - Define Models and Classes
class GenerationRequest(BaseModel):
    prompt: str
    max_tokens: int = 256
    temperature: float = 0.7
    top_p: float = 0.9
    stream: bool = False

class GenerationResponse(BaseModel):
    text: str
    generation_time: float

@dataclass
class BatchItem:
    def __init__(self, prompt, max_tokens, temperature, top_p, future):
        self.prompt = prompt
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.future = future

# Cell 4 - Initialize FastAPI and Global Variables
app = FastAPI(title="Medusa LLM Service")

BATCH_SIZE = 8
BATCH_TIMEOUT = 0.1  # seconds
request_queue = asyncio.Queue()
executor = ThreadPoolExecutor(max_workers=4)

def initialize_models():
    try:
        # Initialize model with Medusa config
        model = MedusaModel.from_pretrained(
            "FasterDecoding/medusa-vicuna-7b-v1.3",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map="auto"
        )

        tokenizer = model.get_tokenizer()

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return model, tokenizer
    except Exception as e:
        print(f"Model initialization error: {e}")
        raise

# Cell 6 - Initialize Model
print("Initializing model... This may take a few minutes...")
model, tokenizer = initialize_models()
print("Model initialized successfully!")

# Backend Processing Functions
from queue import Queue, Empty

async def process_batch(batch: List[BatchItem]):
    try:
        start_time = time.time()

        for i, item in enumerate(batch):
            with torch.inference_mode():
                input_ids = torch.as_tensor([tokenizer.encode(item.prompt)]).cuda()

                generations = model.medusa_generate(
                    input_ids,
                    max_steps=item.max_tokens,
                    temperature=item.temperature,
                    medusa_choices=mc_sim_7b_63,
                    top_p=item.top_p
                )

                generated_text = ""
                for output in generations:
                    generated_text = output["text"]

                generation_time = time.time() - start_time

                item.future.set_result({
                    "text": generated_text,
                    "generation_time": generation_time
                })

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

    except Exception as e:
        for item in batch:
            item.future.set_exception(e)

async def batch_processor():
    print("Batch processor started")
    while True:
        batch = []
        try:
            print("Waiting for first item...")
            # first_item = await asyncio.to_thread(request_queue.get)
            first_item = await request_queue.get()
            print("Got first item")
            batch.append(first_item)

            batch_deadline = time.time() + BATCH_TIMEOUT
            while len(batch) < BATCH_SIZE and time.time() < batch_deadline:
                try:
                    item = request_queue.get_nowait()
                    batch.append(item)
                except asyncio.queues.QueueEmpty:  # Use asyncio.queues.QueueEmpty
                    await asyncio.sleep(0.1)
                    continue

            print(f"Processing batch with {len(batch)} items...")
            await process_batch(batch)

        except Exception as e:
            print(f"Error in batch processing: {e}")
            for item in batch:
                item.future.set_exception(e)

# Cell 8 - FastAPI Endpoints
@app.on_event("startup")
async def startup_event():
    asyncio.create_task(batch_processor())

@app.post("/generate", response_model=GenerationResponse)
async def generate(request: GenerationRequest):
    print(f"Received request with prompt: {request.prompt}")
    future = asyncio.Future()

    item = BatchItem(
        prompt=request.prompt,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
        top_p=request.top_p,
        future=future
    )

    print("Putting item in queue...")
    await request_queue.put(item)
    print("Item placed in queue")

    print("Waiting for result...")
    result = await future
    print(f"Got result: {result}")

    return GenerationResponse(**result)

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# Cell 9 - Setup ngrok and Start Server
# Note: You need to sign up for ngrok and get an auth token
!ngrok authtoken 2o8EyZCZfWRWEc1nnKDe0ORtAJ4_6sgfuHGZWaek4cuZL7uS1
public_url = ngrok.connect(8000)
print(f"Public URL: {public_url}")

if __name__ == "__main__":
    config = uvicorn.Config(app, port=8000, log_level="info")
    server = uvicorn.Server(config)
    server.run()

# Run this in VSCode while above cell running. Colab doesnt allow multiple cells to run simultaneously
import requests

def test_api():
    # test_prompt = "Once upon a time"
    test_prompt = "Donald Trump is"
    url = "https://c395-34-82-224-117.ngrok-free.app/generate" # Copy paste the ngrok URL here / not a neat way to do it but it works

    try:
        response = requests.post(
            url,
            json={
                "prompt": test_prompt,
                "max_tokens": 50,
                "temperature": 0.7,
                "top_p": 0.9
            },
            timeout=300
        )

        # Print response details for debugging
        print(f"Status Code: {response.status_code}")
        print(f"Response Headers: {response.headers}")
        print(f"Raw Response: {response.text}")

        # Check if response is successful
        response.raise_for_status()

        try:
            return response.json()
        except requests.exceptions.JSONDecodeError as e:
            print(f"Failed to decode JSON: {e}")
            print(f"Response content: {response.content}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"Request failed: {e}")
        return None

# Test the API
result = test_api()
if result:
    print("\nParsed JSON Response:", result)











